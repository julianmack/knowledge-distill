{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Model Distillation\n",
    "The aim of this task was to distill a small sentiment classifier from a pretrained RoBERTa teacher model (delivered via the `bert-fast` library). For the student model, GloVe embeddings were fed into an LSTM based model. All training commands (given below) are excecuted via the command line - this notebook is to present results rather than train the models but if you have checkpoints available you can run evaluation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Architecture\n",
    "The sudent architecture design-choices were as follows:\n",
    "* Avoid attention-based residual blocks/models as this would significantly increase student inference time which defeats the point of the distillation.\n",
    "* Use LSTM model for sequence processing - hidden sizes chosen such that there are < `1M` parameters.\n",
    "* For final state take concatenation of max-pool over input sequence and the hidden state of the final token in the sequence. Note that the latter of these requires that we keep track of token sequence lengths.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student from scratch\n",
    "To train the student from scratch run the following command:\n",
    "\n",
    "```bash\n",
    "   python distill/train/train_student.py \\\n",
    "        --expt_name from-scratch \\\n",
    "        --input_csv input-csv-containing-labelled-text.csv \n",
    "        --model_type lstm\n",
    " \n",
    "```\n",
    "\n",
    "This will train and evaluate the model on a 60:20 subset of `--input_csv` (the final 20% is set aside as a test-set).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation\n",
    "To distill a provided `bert-fast` model directory into a randomly initialized student model run the following command:\n",
    "\n",
    "```bash\n",
    "   python distill/train/distill_from_teacher.py \\\n",
    "        --expt_name teacher \\\n",
    "        --input_csv input-csv-containing-unlabelled-text.csv \n",
    "        --model_type lstm\n",
    "```\n",
    "This will train the student model on the teacher's softmax outputs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Potential Optimization\n",
    "The distillation is slow - unecessarily so as I am re-generating the teacher outputs on each epoch. A simple optimization would be to preprocess these once at the start of training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "I will load local versions of these models to generate the results below - to run the cells below it will be necessary to train and manually select the best epoch on the validation set after viewing the model printouts. I will evaluate both models on my held-out test set of labelled headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier_100.pt  LSTMClassifier_40.pt  LSTMClassifier_80.pt\r\n",
      "LSTMClassifier_10.pt   LSTMClassifier_50.pt  LSTMClassifier_90.pt\r\n",
      "LSTMClassifier_20.pt   LSTMClassifier_60.pt\r\n",
      "LSTMClassifier_30.pt   LSTMClassifier_70.pt\r\n"
     ]
    }
   ],
   "source": [
    "ls logs/lstm9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_from_scratch_fp = './logs/lstm6/LSTMClassifier_90.pt'\n",
    "student_distilled_fp = './logs/distill/lstm1/LSTMClassifier_9.pt'\n",
    "teacher_dir = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse \n",
    "import copy \n",
    "import time \n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from distill.evaluate import evaluate, print_eval_res\n",
    "from distill.train.train_student import add_train_args, train_init\n",
    "from distill.data import CSVTextDataset\n",
    "from distill.labels import probs_to_labels, all_labels\n",
    "from distill.teacher import TeacherNLPClassifier\n",
    "from distill.train.train_teacher import unpack_batch_send_to_device as unpack_batch_teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_and_load_student_model(ckpt_path, model_type='lstm'):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = add_train_args(parser)\n",
    "    args = parser.parse_args()\n",
    "    args.model_type = model_type\n",
    "    student_dict = train_init(args)\n",
    "    student = student_dict['model']\n",
    "    student.load_state_dict(torch.load(ckpt_path)['model'])\n",
    "    return student_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_dict = init_and_load_student_model(student_from_scratch_fp)\n",
    "distilled_dict = init_and_load_student_model(student_distilled_fp)\n",
    "teacher = TeacherNLPClassifier(teacher_dir)\n",
    "\n",
    "test_loader = scratch_dict['test_loader']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate student trained from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Accuracy:       \tav=77.6%  \n",
      "F1 Scores:      \tnegative=0.625  neutral=0.845  positive=0.675  av=0.715  av_weight=0.772  micro=0.776  \n",
      "Confusion [negative,neutral,positive]\n",
      "[[ 70  36   8]\n",
      " [ 20 516  51]\n",
      " [ 20  82 167]]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(\n",
    "    **scratch_dict, \n",
    "    loader=test_loader, \n",
    "    subset='test',\n",
    "    probs_to_labels=probs_to_labels, \n",
    "    all_labels=all_labels\n",
    ")\n",
    "print_eval_res(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate distilled student\n",
    "Distilled student will be evaluated on x2 validation subsets: \n",
    "1. The one used above to enable comparisons with the student trained from scratch\n",
    "2. The validation subset used when the teacher was trained to facilitate comparisons with the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Accuracy:       \tav=83.5%  \n",
      "F1 Scores:      \tnegative=0.742  neutral=0.884  positive=0.759  av=0.795  av_weight=0.832  micro=0.835  \n",
      "Confusion [negative,neutral,positive]\n",
      "[[ 82  26   6]\n",
      " [ 12 536  39]\n",
      " [ 13  64 192]]\n"
     ]
    }
   ],
   "source": [
    "# 1: initial test subset\n",
    "results = evaluate(\n",
    "    **distilled_dict, \n",
    "    loader=test_loader, \n",
    "    subset='test',\n",
    "    probs_to_labels=probs_to_labels, \n",
    "    all_labels=all_labels\n",
    ")\n",
    "print_eval_res(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init teacher's validation loader\n",
    "dataset = CSVTextDataset(csv_file='./data/val.csv', headers=['text', 'negative', 'neutral', 'positive'])\n",
    "teacher_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        collate_fn=dataset.collate_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Accuracy:       \tav=80.9%  \n",
      "F1 Scores:      \tnegative=0.766  neutral=0.859  positive=0.715  av=0.780  av_weight=0.807  micro=0.809  \n",
      "Confusion [negative,neutral,positive]\n",
      "[[ 72  21   3]\n",
      " [  8 378  45]\n",
      " [ 12  50 138]]\n",
      "Time taken for evaluation = 1.810s\n"
     ]
    }
   ],
   "source": [
    "# 2: teacher validation's set for distilled student \n",
    "t1 = time.time()\n",
    "results = evaluate(\n",
    "    **distilled_dict, \n",
    "    loader=teacher_dataloader, \n",
    "    subset='test',\n",
    "    probs_to_labels=probs_to_labels, \n",
    "    all_labels=all_labels\n",
    ")\n",
    "t2 = time.time()\n",
    "print_eval_res(results)\n",
    "print(f'Time taken for evaluation = {(t2 - t1):.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Accuracy:       \tav=87.9%  \n",
      "F1 Scores:      \tnegative=0.863  neutral=0.902  positive=0.838  av=0.868  av_weight=0.879  micro=0.879  \n",
      "Confusion [negative,neutral,positive]\n",
      "[[ 82  12   2]\n",
      " [  9 384  38]\n",
      " [  3  24 173]]\n",
      "Time taken for evaluation = 17.568s\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "results = evaluate(\n",
    "    model=teacher,\n",
    "    unpack_batch_fn=unpack_batch_teacher,\n",
    "    loader=teacher_dataloader, \n",
    "    subset='test',\n",
    "    probs_to_labels=probs_to_labels, \n",
    "    all_labels=all_labels\n",
    ")\n",
    "t2 = time.time()\n",
    "print_eval_res(results)\n",
    "print(f'Time taken for evaluation = {(t2 - t1):.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Distillation improved the topline accuracy significantly vs the student trained from scratch. \n",
    "- It also reduced inference time by 10x (on large GPU - improvements on CPU likely greater).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
