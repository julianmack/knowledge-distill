{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Model Distillation\n",
    "The aim of this task was to distill a small sentiment classifier from a pretrained RoBERTa teacher model (delivered via the `bert-fast` library). For the student model, GloVe embeddings were fed into a variant on the Computer Vision [ResNeXt model](https://arxiv.org/abs/1611.05431) that was updated to accomoadate 1D sequences instead of 2D image inputs. All training commands (given below) are excecuted via the command line - this notebook is to present results rather than train the models but if you have checkpoints available you can run evaluation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Architecture\n",
    "The sudent architecture design-choices were as follows:\n",
    "* Avoid attention-based residual blocks/models as this would significantly increase student inference time which defeats the point of the distillation.\n",
    "* For model trunk use tried-and-tested 2016 resNext model as this should make ~efficient use of parameters.\n",
    "* Use resNext-18 instead of any of ResNeXt-{34, 50, 101, 152}.\n",
    "\n",
    "I used the `torchvision` implementation of ResNeXt from [this link](https://github.com/pytorch/vision/blob/052edcecef3eb0ae9fe9e4b256fa2a488f9f395b/torchvision/models/resnet.py#L14) and made the following changes:\n",
    "1. Move from 2D convolution -> 1D operators to support 1D input sequences instead of 2D images. \n",
    "2. Reduce model size significantly. Specifically reduce base channel sizes of `[64, 128, 256, 512]` to\n",
    "`[16, 32, 64, 128]`. This reduced the number of parameters from 9M -> 0.6M.\n",
    "3. When downsampling the feature map, use maxpool & convs in parallel (instead of just maxpool) and concatenate. This also increases feature map size (previous defaults collapsed most sequences to length of 1 very early in the network).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student from scratch\n",
    "To train the student from scratch run the following command:\n",
    "\n",
    "```bash\n",
    "   python distill/train/train_student.py \\\n",
    "        --expt_name from-scratch \\\n",
    "        --input_csv input-csv-containing-labelled-text.csv \n",
    " \n",
    "```\n",
    "\n",
    "This will train and evaluate the model on a 60:20 subset of `--input_csv` (the final 20% is set aside as a test-set).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation\n",
    "To distill a provided `bert-fast` model directory into a randomly initialized student model run the following command:\n",
    "\n",
    "```bash\n",
    "   python distill/train/distill_from_teacher.py \\\n",
    "        --expt_name teacher \\\n",
    "        --input_csv input-csv-containing-unlabelled-text.csv \n",
    "```\n",
    "This will train the student model on the teacher's softmax outputs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Potential Optimization\n",
    "The distillation is slow - unecessarily so as I am re-generating the teacher outputs on each epoch. A simple optimization would be to preprocess these once at the start of training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "I will load local versions of these models to generate the results below - to run the cells below it will be necessary to train and manually select the best epoch on the validation set after viewing the model printouts. I will evaluate both models on my held-out test set of labelled headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/julian/challenges/permutable-test\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_from_scratch_fp = './logs/n5/ConvClassifier_90.pt'\n",
    "student_distilled_fp = './logs/distill/distill2/ConvClassifier_7.pt'\n",
    "teacher_dir = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse \n",
    "import copy \n",
    "import time \n",
    "\n",
    "import torch \n",
    "\n",
    "from distill.evaluate import evaluate, print_eval_res\n",
    "from distill.train.train_student import add_train_args, train_init\n",
    "from distill.labels import probs_to_labels, all_labels\n",
    "from distill.teacher import TeacherNLPClassifier\n",
    "from distill.train.train_teacher import unpack_batch_send_to_device as unpack_batch_teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_and_load_student_model(ckpt_path):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = add_train_args(parser)\n",
    "    args = parser.parse_args()\n",
    "    student_dict = train_init(args)\n",
    "    student = student_dict['model']\n",
    "    student.load_state_dict(torch.load(ckpt_path)['model'])\n",
    "    return student_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_dict = init_and_load_student_model(student_from_scratch_fp)\n",
    "distilled_dict = init_and_load_student_model(student_distilled_fp)\n",
    "teacher = TeacherNLPClassifier(teacher_dir)\n",
    "\n",
    "test_loader = scratch_dict['test_loader']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate student trained from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Accuracy:       \tav=64.7%  \n",
      "F1 Scores:      \tnegative=0.351  neutral=0.782  positive=0.431  av=0.521  av_weight=0.634  micro=0.647  \n",
      "Confusion [negative,neutral,positive]\n",
      "[[ 37  47  30]\n",
      " [ 20 487  80]\n",
      " [ 40 125 104]]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(\n",
    "    **scratch_dict, \n",
    "    loader=test_loader, \n",
    "    subset='test',\n",
    "    probs_to_labels=probs_to_labels, \n",
    "    all_labels=all_labels\n",
    ")\n",
    "print_eval_res(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate distilled student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Accuracy:       \tav=74.8%  \n",
      "F1 Scores:      \tnegative=0.574  neutral=0.846  positive=0.553  av=0.658  av_weight=0.733  micro=0.748  \n",
      "Confusion [negative,neutral,positive]\n",
      "[[ 66  35  13]\n",
      " [ 15 540  32]\n",
      " [ 35 114 120]]\n",
      "Time taken for evaluation = 2.284s\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "results = evaluate(\n",
    "    **distilled_dict, \n",
    "    loader=test_loader, \n",
    "    subset='test',\n",
    "    probs_to_labels=probs_to_labels, \n",
    "    all_labels=all_labels\n",
    ")\n",
    "t2 = time.time()\n",
    "print_eval_res(results)\n",
    "print(f'Time taken for evaluation = {(t2 - t1):.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Accuracy:       \tav=93.3%  \n",
      "F1 Scores:      \tnegative=0.902  neutral=0.947  positive=0.915  av=0.921  av_weight=0.933  micro=0.933  \n",
      "Confusion [negative,neutral,positive]\n",
      "[[101  12   1]\n",
      " [  7 555  25]\n",
      " [  2  18 249]]\n",
      "Time taken for evaluation = 23.307s\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "results = evaluate(\n",
    "    model=teacher,\n",
    "    unpack_batch_fn=unpack_batch_teacher,\n",
    "    loader=test_loader, \n",
    "    subset='test',\n",
    "    probs_to_labels=probs_to_labels, \n",
    "    all_labels=all_labels\n",
    ")\n",
    "t2 = time.time()\n",
    "print_eval_res(results)\n",
    "print(f'Time taken for evaluation = {(t2 - t1):.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "- Distillation improved the topline accuracy by 10%. \n",
    "- It also reduced inference time by 10x (on large GPU - improvements on CPU likely greater).\n",
    "- There is still significant room for improvement for the `distilled` model vs the `teacher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
